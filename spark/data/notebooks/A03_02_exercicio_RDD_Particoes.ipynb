{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor:** Tatiana Massae Sakata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a name=\"modulo06\"> Módulo Spark </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"inicio\"> Exercicios Aula 3 - RDD Partições </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Inicio](#inicio)\n",
    "* [Questão 01](#questao01)\n",
    "* [Questão 02](#questao02)\n",
    "* [Questão 03](#questao03)\n",
    "* [Questão 04](#questao04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a name=\"questao01\"> Questão 1. Ler com RDD os arquivos localmente do diretório “/opt/spark/logs/” (\"file:///opt/spark/logs/\") com 10 partições </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_particoes = sc.textFile(\"file:///opt/spark/logs\",10)\n",
    "\n",
    "log_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a name=\"questao02\"> Questão 2. Contar a quantidade de cada palavras do RDD em 5 partições </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separar as palavras da linha\n",
    "\n",
    "palavras_particoes = log_particoes.flatMap(lambda linha: linha.split(\" \"),5)\n",
    "palavras_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convertendo as palavras em minuscula\n",
    "palavras_minuscula_particoes = palavras_particoes.map(lambda palavra: palavra.lower(),8)\n",
    "\n",
    "#Validando\n",
    "palavras_minuscula_particoes.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizando as repartições\n",
    "palavras_minuscula_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contar as palavras com os mesmos nomes\n",
    "palavra_count_particoes = palavras_minuscula_particoes.map(lambda palavra: (palavra,1),7)\n",
    "palavra_count_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enfim, atribuindo a 5 partições\n",
    "palavras_reduce_particoes = palavra_count_particoes.reduceByKey(lambda x,y:x+y,5)\n",
    "palavras_reduce_particoes.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a name=\"questao03\"> Questão 3. Salvar o RDD no diretório do HDFS /user/<seu-nome>/logs_count_word_5 </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_reduce_particoes.saveAsTextFile(\"/user/tatiana/logs_count_word_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-11-10 18:07 /user/tatiana/logs_count_word_5/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        575 2022-11-10 18:07 /user/tatiana/logs_count_word_5/part-00000\r\n",
      "-rw-r--r--   2 root supergroup        208 2022-11-10 18:07 /user/tatiana/logs_count_word_5/part-00001\r\n",
      "-rw-r--r--   2 root supergroup        444 2022-11-10 18:07 /user/tatiana/logs_count_word_5/part-00002\r\n",
      "-rw-r--r--   2 root supergroup        772 2022-11-10 18:07 /user/tatiana/logs_count_word_5/part-00003\r\n",
      "-rw-r--r--   2 root supergroup        785 2022-11-10 18:07 /user/tatiana/logs_count_word_5/part-00004\r\n"
     ]
    }
   ],
   "source": [
    "#Para verificar se salvou\n",
    "\n",
    "!hdfs dfs -ls /user/tatiana/logs_count_word_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <a name=\"questao04\"> Questão 4. Refazer a questão 2, com todas as funções na mesma linha de um RDD </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_particoes_linha = log_particoes.flatMap(lambda linha: linha.split(\" \")).map(lambda palavra: palavra.lower()).map(lambda palavra: (palavra,1)).reduceByKey(lambda x,y:x+y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('--port', 1),\n",
       " ('20:31:09', 4),\n",
       " ('daemon', 1),\n",
       " ('process', 1),\n",
       " ('registered', 3),\n",
       " ('signal', 3),\n",
       " ('20:31:10', 24),\n",
       " ('to:', 4),\n",
       " ('groups', 4),\n",
       " ('', 4),\n",
       " ('authentication', 1),\n",
       " ('users', 2),\n",
       " ('permissions:', 4),\n",
       " ('on', 2),\n",
       " ('starting', 1),\n",
       " ('at', 2),\n",
       " ('2.4.1', 1),\n",
       " ('util.log:', 1),\n",
       " ('timestamp:', 1),\n",
       " ('@1504ms', 1),\n",
       " (\"'masterui'\", 1),\n",
       " ('o.s.j.s.servletcontexthandler@2a362185{/app/kill,null,available,@spark}',\n",
       "  1),\n",
       " ('o.s.j.s.servletcontexthandler@222db61{/driver/kill,null,available,@spark}',\n",
       "  1),\n",
       " ('to', 1),\n",
       " ('0.0.0.0,', 1),\n",
       " ('http://jupyter-notebook:8080', 1),\n",
       " ('have', 1),\n",
       " ('elected', 1),\n",
       " ('alive', 1),\n",
       " ('command:', 1),\n",
       " ('-cp', 1),\n",
       " ('info', 28),\n",
       " ('int', 1),\n",
       " ('acls', 5),\n",
       " ('modify', 4),\n",
       " ('util.utils:', 2),\n",
       " ('7077.', 1),\n",
       " ('master', 1),\n",
       " ('jetty-9.3.z-snapshot,', 1),\n",
       " ('8080.', 1),\n",
       " ('bound', 1),\n",
       " ('masterwebui', 1),\n",
       " ('and', 1),\n",
       " ('/opt/java/bin/java', 1),\n",
       " ('org.apache.spark.deploy.master.master', 1),\n",
       " ('8080', 1),\n",
       " ('20/03/18', 28),\n",
       " ('name:', 1),\n",
       " ('util.signalutils:', 3),\n",
       " ('handler', 3),\n",
       " ('for', 3),\n",
       " ('successfully', 2),\n",
       " ('service', 2),\n",
       " ('running', 1),\n",
       " ('version', 1),\n",
       " ('logging', 1),\n",
       " ('@1454ms', 1),\n",
       " ('server.server:', 2),\n",
       " ('unknown', 1),\n",
       " ('server.abstractconnector:', 1),\n",
       " ('o.s.j.s.servletcontexthandler@44d0ea1e{/static,null,available,@spark}', 1),\n",
       " ('been', 1),\n",
       " ('leader!', 1),\n",
       " ('new', 1),\n",
       " ('/etc/spark/:/opt/spark/jars/*:/etc/hadoop/:/etc/hadoop/*:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/tools/lib/*',\n",
       "  1),\n",
       " ('localhost', 1),\n",
       " ('--webui-port', 1),\n",
       " ('started', 15),\n",
       " ('changing', 4),\n",
       " ('securitymanager:', 1),\n",
       " ('disabled;', 2),\n",
       " ('set(root);', 2),\n",
       " ('set();', 1),\n",
       " (\"'sparkmaster'\", 1),\n",
       " ('spark://localhost:7077', 1),\n",
       " ('initialized', 1),\n",
       " ('git', 1),\n",
       " ('hash:', 1),\n",
       " ('o.s.j.s.servletcontexthandler@78c62b96{/app,null,available,@spark}', 1),\n",
       " ('o.s.j.s.servletcontexthandler@50e5c33c{/json,null,available,@spark}', 1),\n",
       " ('spark', 3),\n",
       " ('-xmx1g', 1),\n",
       " ('--host', 1),\n",
       " ('7077', 1),\n",
       " ('========================================', 1),\n",
       " ('master.master:', 4),\n",
       " ('with', 5),\n",
       " ('1087@jupyter-notebook', 1),\n",
       " ('term', 1),\n",
       " ('hup', 1),\n",
       " ('spark.securitymanager:', 5),\n",
       " ('view', 4),\n",
       " ('root', 2),\n",
       " ('ui', 1),\n",
       " ('set()', 1),\n",
       " ('port', 2),\n",
       " ('build', 1),\n",
       " ('unknown,', 1),\n",
       " ('serverconnector@5526ec85{http/1.1,[http/1.1]}{0.0.0.0:8080}', 1),\n",
       " ('handler.contexthandler:', 9),\n",
       " ('o.s.j.s.servletcontexthandler@1330e23c{/app/json,null,available,@spark}',\n",
       "  1),\n",
       " ('o.s.j.s.servletcontexthandler@3f7fb168{/,null,available,@spark}', 1),\n",
       " ('ui.masterwebui:', 1),\n",
       " ('o.s.j.s.servletcontexthandler@58cb287e{/metrics/master/json,null,available,@spark}',\n",
       "  1),\n",
       " ('o.s.j.s.servletcontexthandler@3d2d7a89{/metrics/applications/json,null,available,@spark}',\n",
       "  1),\n",
       " ('i', 1),\n",
       " ('state:', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando\n",
    "\n",
    "palavras_particoes_linha.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    IMPORTANTE. \n",
    "    Vá na aba “Kernel” e aperte “Shutdown” para encerrar a sessão.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
