{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Tatiana Massae Sakata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Módulo Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"inicio\"> Exercicios Aula 8 - Structured Streaming </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/tatiana/data/iris/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/tatiana/data/iris/*.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.csv(\"/user/tatiana/data/iris/*.data\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.csv(\"/user/tatiana/data/iris/bezdekIris.data\")\n",
    "iris.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.csv(\"/user/tatiana/data/iris/iris.data\")\n",
    "iris.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais fácil utilizar o .add do que o StructField\n",
    "\n",
    "iris_schema = StructType()\\\n",
    "    .add(\"sepal_lenght\", \"float\")\\\n",
    "    .add(\"sepal_width\", \"float\")\\\n",
    "    .add(\"pepal_lenght\", \"float\")\\\n",
    "    .add(\"pepal_width\", \"float\")\\\n",
    "    .add(\"class\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obrigatoriamente trabalhar com o Stream, precisa ter um cchema\n",
    "\n",
    "iris = spark.read.schema(iris_schema).csv(\"/user/tatiana/data/iris/*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.schema(iris_schema).csv(\"/user/tatiana/data/iris/*.data\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.readStream.schema(iris_schema).csv(\"/user/tatiana/data/iris/*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependendo do formato não é necessário o checkpoint\n",
    "\n",
    "iris_saida = iris.writeStream.format(\"csv\")\\\n",
    "    .option(\"checkpointLocation\", \"/user/tatiana/stream_iris/check\")\\\n",
    "    .option(\"path\", \"/user/tatiana/stream_iris/path\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_saida.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_saida.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/tatiana/stream_iris/check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/tatiana/stream_iris/path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"/user/tatiana/stream_iris/path/part-00000-f62c8ae9-a4ee-45d3-a32b-ca6390505d98-c000.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"/user/tatiana/stream_iris/path/part-00000-f62c8ae9-a4ee-45d3-a32b-ca6390505d98-c000.csv\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Inicio - Parte 1](#inicio)\n",
    "* [Questão 01](#questao01)\n",
    "* [Questão 02](#questao02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"questao01\"> Questão 1. Instalar o NetCat no container do spark </a>\n",
    "\n",
    "<a> - apt update </a>\n",
    "\n",
    "<a> - apt install netcat  </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root@jupyter-spark:/# apt install netcat\n",
    "Reading package lists... Done\n",
    "Building dependency tree\n",
    "Reading state information... Done\n",
    "The following additional packages will be installed:\n",
    "  netcat-traditional\n",
    "The following NEW packages will be installed:\n",
    "  netcat netcat-traditional\n",
    "0 upgraded, 2 newly installed, 0 to remove and 54 not upgraded.\n",
    "Need to get 76.0 kB of archives.\n",
    "After this operation, 173 kB of additional disk space will be used.\n",
    "Do you want to continue? [Y/n] y\n",
    "Get:1 http://deb.debian.org/debian stretch/main amd64 netcat-traditional amd64 1.10-41+b1 [67.0 kB]\n",
    "Get:2 http://deb.debian.org/debian stretch/main amd64 netcat all 1.10-41 [8962 B]\n",
    "Fetched 76.0 kB in 0s (722 kB/s)\n",
    "debconf: delaying package configuration, since apt-utils is not installed\n",
    "Selecting previously unselected package netcat-traditional.\n",
    "(Reading database ... 10795 files and directories currently installed.)\n",
    "Preparing to unpack .../netcat-traditional_1.10-41+b1_amd64.deb ...\n",
    "Unpacking netcat-traditional (1.10-41+b1) ...\n",
    "Selecting previously unselected package netcat.\n",
    "Preparing to unpack .../netcat_1.10-41_all.deb ...\n",
    "Unpacking netcat (1.10-41) ...\n",
    "Setting up netcat-traditional (1.10-41+b1) ...\n",
    "update-alternatives: using /bin/nc.traditional to provide /bin/nc (nc) in auto mode\n",
    "Setting up netcat (1.10-41) ...\n",
    "root@jupyter-spark:/# exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a name=\"questao02\"> Questão 2. Alterar o formato do campo data para “MM/dd/yyyy” </a>\n",
    "\n",
    "[voltar ao ínicio](#inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se estivéssemos utilizando a IDE teriamos que realizar as configurações desta linha\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"DStream Python\")\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#porta de entrada de TCP para recebimento de dados\n",
    "dstream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstream.pprint()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Entrada de informações no terminal PowerShell:\n",
    "\n",
    "PS C:\\> docker exec -it jupyter-spark bash\n",
    "root@jupyter-spark:/# nc -h\n",
    "[v1.10-41+b1]\n",
    "connect to somewhere:   nc [-options] hostname port[s] [ports] ...\n",
    "listen for inbound:     nc -l -p port [-options] [hostname] [port]\n",
    "options:\n",
    "        -c shell commands       as `-e'; use /bin/sh to exec [dangerous!!]\n",
    "        -e filename             program to exec after connect [dangerous!!]\n",
    "        -b                      allow broadcasts\n",
    "        -g gateway              source-routing hop point[s], up to 8\n",
    "        -G num                  source-routing pointer: 4, 8, 12, ...\n",
    "        -h                      this cruft\n",
    "        -i secs                 delay interval for lines sent, ports scanned\n",
    "        -k                      set keepalive option on socket\n",
    "        -l                      listen mode, for inbound connects\n",
    "        -n                      numeric-only IP addresses, no DNS\n",
    "        -o file                 hex dump of traffic\n",
    "        -p port                 local port number\n",
    "        -r                      randomize local and remote ports\n",
    "        -q secs                 quit after EOF on stdin and delay of secs\n",
    "        -s addr                 local source address\n",
    "        -T tos                  set Type Of Service\n",
    "        -t                      answer TELNET negotiation\n",
    "        -u                      UDP mode\n",
    "        -v                      verbose [use twice to be more verbose]\n",
    "        -w secs                 timeout for connects and final net reads\n",
    "        -C                      Send CRLF as line-ending\n",
    "        -z                      zero-I/O mode [used for scanning]\n",
    "port numbers can be individual or ranges: lo-hi [inclusive];\n",
    "hyphens in port names must be backslash escaped (e.g. 'ftp\\-data').\n",
    "root@jupyter-spark:/# nc -l -p 9999\n",
    "tatiana\n",
    "eder\n",
    "maria\n",
    "tatiana\n",
    "eder\n",
    "maria\n",
    "alice\n",
    "pão\n",
    "acucar\n",
    "root@jupyter-spark:/#\n",
    "\n",
    "\n",
    "#após a configuração do Sleep - após 4 rodadas de 5 segundos.\n",
    "root@jupyter-spark:/# nc -l -p 9999\n",
    "tatiana\n",
    "eder\n",
    "maria\n",
    "root@jupyter-spark:/#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "sleep(20)\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    IMPORTANTE. \n",
    "    Vá na aba “Kernel” e aperte “Shutdown” para encerrar a sessão.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
